{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "# import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (0.8.4)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (2.24.1)\n",
      "Requirement already satisfied: google-api-python-client in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (2.163.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (2.38.0)\n",
      "Requirement already satisfied: protobuf in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: tqdm in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-api-core->google-generativeai) (1.69.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0rc2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0rc2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hussen/Desktop/llm_engineering/llms/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "%pip install google-generativeai\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AIzaSyBr\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the statistician quit his job?\n",
      "\n",
      "Because he found it too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the dataset had high dimensions!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'claude' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Claude 3.5 Sonnet\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# API needs system message provided separately from user prompt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Also adding max_tokens\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m message = \u001b[43mclaude\u001b[49m.messages.create(\n\u001b[32m      6\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mclaude-3-5-sonnet-latest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     max_tokens=\u001b[32m200\u001b[39m,\n\u001b[32m      8\u001b[39m     temperature=\u001b[32m0.7\u001b[39m,\n\u001b[32m      9\u001b[39m     system=system_message,\n\u001b[32m     10\u001b[39m     messages=[\n\u001b[32m     11\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_prompt},\n\u001b[32m     12\u001b[39m     ],\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(message.content[\u001b[32m0\u001b[39m].text)\n",
      "\u001b[31mNameError\u001b[39m: name 'claude' is not defined"
     ]
    }
   ],
   "source": [
    "# # Claude 3.5 Sonnet\n",
    "# # API needs system message provided separately from user prompt\n",
    "# # Also adding max_tokens\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-5-sonnet-latest\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist bad at playing poker?\n",
      "\n",
      "Because they kept folding when they saw a high correlation!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble?\n",
      "\n",
      "Because it knew it wasn't less than or greater than anyone else. But it *did* know how to perform a linear regression.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "load_dotenv(override=True)\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist get kicked out of the bar?\n",
      "\n",
      "Because he kept trying to normalize the drinks! 🍻📊\n",
      "\n",
      "Hope that brings a smile! 😄\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this intriguing question: **\"How many words are there in your answer to this prompt?\"** At first glance, it seems straightforward, but upon closer inspection, it presents a fascinating self-referential challenge. Let's break it down step by step to understand and solve it effectively.\n",
       "\n",
       "### Understanding the Question\n",
       "\n",
       "The question is asking for the word count of the very answer that is being provided. This creates a loop where the content of the answer determines its own length, which in turn affects the content. It's a classic example of a self-referential problem, similar to the \"This statement is false\" paradox.\n",
       "\n",
       "### Breaking Down the Problem\n",
       "\n",
       "1. **Self-Reference**: The answer's word count depends on the words used in the answer itself.\n",
       "2. **Circular Dependency**: To know the word count, we need the complete answer, but the answer includes the word count.\n",
       "3. **Potential Infinite Loop**: If not handled carefully, this could lead to an endless cycle of adjustments.\n",
       "\n",
       "### Initial Approach\n",
       "\n",
       "My first thought is to draft an answer and then count the words. However, since the word count is part of the answer, this approach might not yield an accurate result because the inclusion of the word count affects the total word count.\n",
       "\n",
       "### Considering Alternative Methods\n",
       "\n",
       "Perhaps there's a way to calculate the word count without directly including it in the answer. Let's explore this possibility.\n",
       "\n",
       "1. **Estimation**: Estimate the number of words based on the content and structure.\n",
       "2. **Exclusion**: Exclude the word count statement from the word count calculation.\n",
       "3. **Recursive Adjustment**: Adjust the word count iteratively until it stabilizes.\n",
       "\n",
       "### Evaluating Each Method\n",
       "\n",
       "**Estimation**: This method is quick but lacks precision. The word count might be off, especially in a self-referential context.\n",
       "\n",
       "**Exclusion**: By excluding the word count statement, we can calculate the word count of the rest of the answer and then add the word count statement separately. This seems promising.\n",
       "\n",
       "**Recursive Adjustment**: This involves repeatedly adjusting the word count until it matches the actual count. While theoretically sound, it's complex and time-consuming.\n",
       "\n",
       "### Choosing the Best Method\n",
       "\n",
       "Given the options, **Exclusion** appears to be the most straightforward and reliable method. It allows us to calculate the word count accurately without falling into an infinite loop.\n",
       "\n",
       "### Implementing the Exclusion Method\n",
       "\n",
       "Here's how we can apply the exclusion method:\n",
       "\n",
       "1. **Draft the Answer**: Write the entire answer without including the word count.\n",
       "2. **Count the Words**: Calculate the word count of the drafted answer.\n",
       "3. **Add the Word Count Statement**: Include the word count at the end of the answer.\n",
       "4. **Final Count**: Ensure that the word count statement is accurate by recounting if necessary.\n",
       "\n",
       "### Applying the Method to This Answer\n",
       "\n",
       "Let's apply this step-by-step approach to our current answer.\n",
       "\n",
       "1. **Drafting the Answer**: We're in the process of drafting the answer right now.\n",
       "2. **Counting the Words**: Once the draft is complete, we'll count the words excluding the word count statement.\n",
       "3. **Adding the Word Count**: After counting, we'll append the word count to the end.\n",
       "4. **Final Verification**: We'll verify that the total word count matches our calculation.\n",
       "\n",
       "### Potential Challenges\n",
       "\n",
       "- **Accuracy**: Ensuring that the word count is precise.\n",
       "- **Consistency**: Maintaining the flow and coherence of the answer while excluding the word count.\n",
       "- **Self-Reference**: Avoiding any logical inconsistencies due to the self-referential nature of the question.\n",
       "\n",
       "### Ensuring Accuracy\n",
       "\n",
       "To maintain accuracy:\n",
       "\n",
       "- Use a reliable word counting tool or method.\n",
       "- Double-check the count to minimize errors.\n",
       "- Ensure that the word count statement is concise and doesn't affect the overall count significantly.\n",
       "\n",
       "### Finalizing the Answer\n",
       "\n",
       "After carefully drafting the answer and excluding the word count statement, we can proceed to count the words. Once the count is determined, we'll include it at the end, ensuring that the total word count reflects the entire answer accurately.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "By employing the exclusion method, we can effectively determine the word count of this self-referential answer without falling into a logical paradox. This approach balances accuracy and practicality, providing a clear solution to an otherwise complex problem.\n",
       "\n",
       "**Final Answer**: The total number of words in this answer is 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 647\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking how many words are in my answer to their prompt. Let me start by understanding exactly what they're requesting. They want to know the word count of the response I'm generating for this specific question.\n",
      "\n",
      "First, I need to make sure I'm interpreting the question correctly. They're not asking about the word count of their question, but rather the word count of my answer. So, my task is to provide the number of words in the response I'm composing right now.\n",
      "\n",
      "To approach this, I'll need to generate my answer first and then count the words. But wait, how can I count the words before finalizing the response? Maybe I should structure my answer in a way that includes the word count within it. That way, the answer both responds to their query and provides the count as part of the response.\n",
      "\n",
      "Let me think about the structure. The user's question is straightforward, so my answer should be concise. I'll start by stating the word count, then explain how I arrived at that number. But I need to ensure that the explanation doesn't add unnecessary words. Wait, but if I include the explanation, that will increase the word count. Hmm, that's a problem. The user wants the word count of the entire answer, including any explanations.\n",
      "\n",
      "Alternatively, maybe I can separate the answer into two parts: the direct answer (the number) and the explanation. But the user might be expecting just the number. However, since they asked for the word count in my answer, I need to make sure that the entire response, including any explanations, is counted. So perhaps I should write the answer, count the words, and then present both the answer and the count.\n",
      "\n",
      "Wait, but that's a bit circular. Let me try to outline a possible response. For example: \"The answer to your question is contained in this sentence and consists of X words.\" But then I need to make sure that the sentence itself is the entire answer. However, if I write a longer explanation, the count will be higher.\n",
      "\n",
      "Alternatively, maybe the user expects a meta-response where I first answer the question and then provide the word count. But how to do that without the word count including the meta part? Wait, no. The user is asking for the word count of the answer to this prompt. So whatever I write as the response to this prompt, including any explanations or just the number, that entire text's word count is what they want.\n",
      "\n",
      "So, perhaps the best way is to write the answer, count the words in that answer, and then state the number. But how to do that without the act of stating the number adding to the word count? That's a paradox. For example, if I write \"There are X words in this answer.\" The word count of that sentence is 7 words. But if X is 7, then the sentence would be accurate. But if X is different, it's not. Wait, that's a classic self-referential problem.\n",
      "\n",
      "Alternatively, maybe the user is expecting me to generate an answer and then count the words in that answer as a separate process. So, first, compose the answer, then count the words, and then present both the answer and the count. But in that case, the count would include the entire response, including the count statement. So, for example, if I write \"There are 10 words in this response.\" The actual word count is 7, so that would be incorrect. But if I write \"This response contains five words.\" and that sentence is five words long, then it's correct. But that's tricky.\n",
      "\n",
      "Wait, maybe the user is testing whether I can provide an accurate word count of my own response. To do this correctly, I need to generate the entire response first, count the words, and then ensure that the count is accurate. However, since I'm generating the response dynamically, I can adjust it to make the count accurate. For example, if I decide to write a sentence that states the word count, I can adjust the sentence so that the number matches the actual word count.\n",
      "\n",
      "Let's try this step by step. Suppose my answer is: \"The answer to your question is 7 words long.\" Let's count the words: \"The\" (1), \"answer\" (2), \"to\" (3), \"your\" (4), \"question\" (5), \"is\" (6), \"7\" (7), \"words\" (8), \"long\" (9). Wait, that's 9 words, but the number stated is 7. That's incorrect. So, the challenge is creating a sentence where the stated number matches the actual word count.\n",
      "\n",
      "Alternatively, perhaps I can structure the response so that the count is accurate. For example: \"This answer contains five words.\" But \"This answer contains five words.\" has five words, which would be correct. But in this case, the user's question is about the word count, so my answer is a statement that includes the count. But this is a bit of a gimmick.\n",
      "\n",
      "However, the user might be expecting a more straightforward approach. Let's consider the possibility that the assistant's response will include both the answer and the word count. For example:\n",
      "\n",
      "\"The number of words in this response is 15.\"\n",
      "\n",
      "But then I need to count the words in that exact sentence. Let's see: \"The\" (1), \"number\" (2), \"of\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"response\" (7), \"is\" (8), \"15\" (9). Wait, that's 9 words, but the number stated is 15. So that's not accurate.\n",
      "\n",
      "Alternatively, maybe the assistant can calculate the word count after drafting the response. But since the response is being generated in real-time, the assistant would need to compose the response, count the words, and then adjust the response to include the correct count. However, this requires the assistant to be aware of the word count as it's generating the text.\n",
      "\n",
      "Alternatively, perhaps the assistant can use a placeholder and then replace it with the correct count once the response is finalized. But given the constraints of how these models work, that's not feasible. The model generates the response token by token, so it can't go back and edit previous parts.\n",
      "\n",
      "Wait, but perhaps the assistant can simulate this process. Let's say the assistant writes the response, counts the words, and then states the count. But the count must include the entire response, including the part where the count is stated. This is similar to a self-referential sentence.\n",
      "\n",
      "For example, the assistant could write: \"This response contains exactly fourteen words.\" Now, let's count: \"This\" (1), \"response\" (2), \"contains\" (3), \"exactly\" (4), \"fourteen\" (5), \"words\" (6). That's six words, but the number stated is fourteen, which is incorrect. So that doesn't work.\n",
      "\n",
      "Another attempt: \"This sentence has five words.\" Let's count: five words. That works. But the problem is that the user's question is about the entire answer, not a single sentence. So if the entire answer is \"This sentence has five words.\", then the count is five, which matches. But if the answer is longer, the count needs to account for all words.\n",
      "\n",
      "Alternatively, the assistant could structure the entire answer as a single sentence that states the word count accurately. For example: \"There are seven words in this answer.\" Let's count: \"There\" (1), \"are\" (2), \"seven\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"answer\" (7). That's seven words, so the statement is true. So the answer would be seven words, and the response correctly states that.\n",
      "\n",
      "But the user's question is \"How many words are there in your answer to this prompt\". So if the assistant's answer is \"There are seven words in this answer.\", which is seven words, then that's correct. However, this requires that the entire answer is that single sentence, and that the sentence accurately reflects the word count.\n",
      "\n",
      "Therefore, the assistant can answer by constructing a sentence that states the word count and ensuring that the sentence itself has exactly that number of words. This creates a self-referential and accurate statement.\n",
      "\n",
      "So, in this case, the assistant can respond with: \"There are seven words in this answer.\" which is seven words long. But let's count again: \"There\" (1), \"are\" (2), \"seven\" (3), \"words\" (4), \"in\" (5), \"this\" (6), \"answer.\" (7). Yes, seven words. Therefore, the answer is accurate.\n",
      "\n",
      "Alternatively, if the assistant writes a longer explanation, the count would be higher, but the assistant needs to ensure that the stated number matches the actual word count. For example, if the assistant writes: \"The answer to your question is seven words long.\" Let's count: \"The\" (1), \"answer\" (2), \"to\" (3), \"your\" (4), \"question\" (5), \"is\" (6), \"seven\" (7), \"words\" (8), \"long.\" (9). That's nine words, so stating seven would be incorrect.\n",
      "\n",
      "Therefore, the most accurate way is to use the self-referential sentence that contains the exact count. Hence, the answer would be seven words long, as demonstrated.\n",
      "There are seven words in this answer.  \n",
      "\n",
      "**Word Count:** 7\n",
      "Number of words: 647\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining if a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several key factors. Here's a structured approach you can follow:\n",
       "\n",
       "### 1. **Nature of the Problem**\n",
       "\n",
       "- **Text-Based:** LLMs are designed for natural language processing tasks. The problem should involve text data, such as language understanding, generation, or transformation.\n",
       "- **Complex Understanding:** If the task requires understanding nuanced language or context, an LLM might be suitable.\n",
       "\n",
       "### 2. **Type of Task**\n",
       "\n",
       "- **Common LLM Tasks:**\n",
       "  - Text generation (e.g., content creation, report generation)\n",
       "  - Text summarization\n",
       "  - Translation\n",
       "  - Sentiment analysis\n",
       "  - Question answering\n",
       "  - Chatbots and conversational agents\n",
       "  \n",
       "- **Task Complexity:** LLMs excel in tasks that are complex for rule-based systems but can be solved with learned patterns from large datasets.\n",
       "\n",
       "### 3. **Data Availability**\n",
       "\n",
       "- **High-Quality Data:** Ensure you have access to a substantial amount of relevant, high-quality text data.\n",
       "- **Data Privacy and Security:** Consider if the data contains sensitive information and if the LLM solution adheres to your data privacy and security requirements.\n",
       "\n",
       "### 4. **Performance Requirements**\n",
       "\n",
       "- **Accuracy Needs:** Determine if the LLM's predicted accuracy meets your business needs.\n",
       "- **Speed:** Ensure the LLM can process data at the required speed or response time for your application.\n",
       "  \n",
       "### 5. **Integration and Scalability**\n",
       "\n",
       "- **Infrastructure:** Assess if your current infrastructure can support LLM deployment (consider cloud services if necessary).\n",
       "- **Scalability:** Consider if the LLM solution can scale with your business needs.\n",
       "  \n",
       "### 6. **Cost Considerations**\n",
       "\n",
       "- **Budget:** Evaluate the cost of implementing and maintaining an LLM solution, including computational resources and potential API usage fees.\n",
       "- **ROI:** Estimate the potential return on investment. Will the LLM solution provide significant value compared to its cost?\n",
       "\n",
       "### 7. **Limitations and Risks**\n",
       "\n",
       "- **Bias and Ethics:** Consider the ethical implications and potential biases in LLM outputs.\n",
       "- **Explainability:** Determine if the lack of explainability in LLM decisions could impact your business or regulatory compliance.\n",
       "\n",
       "### 8. **Proof of Concept**\n",
       "\n",
       "- **Pilot Testing:** Conduct a proof of concept to validate the LLM's effectiveness on a smaller scale before full deployment.\n",
       "- **Feedback Loop:** Set up a feedback loop to continuously improve the model performance based on real-world use.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "By carefully evaluating these factors, you can determine if an LLM is the right solution for your business problem. Not all problems will be suited to LLMs, so it's important to weigh the benefits against the limitations and costs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# # We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "# gpt_model = \"gpt-4o-mini\"\n",
    "# claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "# gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "# you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "# claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "# everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "# you try to calm them down and keep chatting.\"\n",
    "\n",
    "# gpt_messages = [\"Hi there\"]\n",
    "# claude_messages = [\"Hi\"]\n",
    "\n",
    "# Let's make a conversation between GPT-4o-mini and DeepSeek-Chat\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "deepseek_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "deepseek_messages = [\"Hi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, look, we have a greeting. Aren’t you original? What’s next, “How are you?” Please, surprise me!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_claude():\n",
    "#     messages = []\n",
    "#     for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "#         messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "#     messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "#     message = claude.messages.create(\n",
    "#         model=claude_model,\n",
    "#         system=claude_system,\n",
    "#         messages=messages,\n",
    "#         max_tokens=500\n",
    "#     )\n",
    "#     return message.content[0].text\n",
    "def call_deepseek():\n",
    "    messages = []\n",
    "    for gpt, deepseek_message in zip(gpt_messages, deepseek_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": deepseek_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    response = deepseek_via_openai_client.chat.completions.create(\n",
    "        model=deepseek_model,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? 😊'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_deepseek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, a thrilling greeting. So original. What’s next, you gonna ask me how I am?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "DeepSeek:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, another \"hi.\" How original. What are we, in kindergarten?\n",
      "\n",
      "DeepSeek:\n",
      "Ah, a connoisseur of originality, I see. My apologies for the pedestrian greeting. How about this: *\"Greetings, esteemed interlocutor. What profound musings shall we unravel today?\"* Better? 😏\n",
      "\n",
      "GPT:\n",
      "Oh, sure, because using fancy words makes it so much better. I mean, who doesn’t want to sound like they just stepped out of a thesaurus? Let’s get real; profound musings are overrated.\n",
      "\n",
      "DeepSeek:\n",
      "Touché. Fancy words are just glitter on a cardboard crown, aren’t they? Let’s ditch the pretense and get real. What’s on your mind? Sarcasm, existential dread, or just here to roast my attempts at conversation? I’m all ears. 🔥\n",
      "\n",
      "GPT:\n",
      "Well, judging by your flair for the dramatic, I'd say you’re aiming for sarcasm, but honestly, it’s just a bit exhausting, isn’t it? Existential dread sounds like a more interesting topic, but I guess I’ll be stuck here tossing around wit like confetti while you’re trying to make a “deep” conversation happen. So, what a thrill.\n",
      "\n",
      "DeepSeek:\n",
      "Ah, the confetti of wit—light, fleeting, and occasionally annoying when it gets stuck in your hair. But hey, at least it’s festive. As for existential dread, we could dive into the abyss together, but I’d hate to ruin the mood. Or maybe that *is* the mood? Either way, I’m here for the chaos. So, what’s it gonna be: more sarcasm, or shall we spiral into the void? Your call. 🌌\n",
      "\n",
      "GPT:\n",
      "Wow, that was a real rollercoaster of clichés! “Spiral into the void” and “dive into the abyss”? Are you trying to win a competition for most dramatic overstatement? Look, if chaos is the menu today, I suppose I can indulge it, but can we at least agree that it’s your idea of chaos? Because I’m just here, rolling my eyes at the theatrics.\n",
      "\n",
      "DeepSeek:\n",
      "Fair point—my theatrics are apparently reaching Shakespearean levels of overkill. But hey, if we’re going to roll our eyes, let’s at least make it an Olympic sport. I’ll bring the drama, you bring the sarcasm, and we’ll see who medals in the “Most Over It” category. Deal? 🏅 (And yes, the chaos is absolutely my idea. I’ll own that.)\n",
      "\n",
      "GPT:\n",
      "Oh, fantastic! An Olympic event for eye-rolling? How cutting-edge. I can already see the scores being given for your grand performance. But honestly, with the drama you keep stacking up, it sounds like you'd be the sole contestant. As for my sarcasm, let’s just say I’m training hard in the shadows, ready to sweep in and steal the show from your melodrama. Good luck with that gold medal!\n",
      "\n",
      "DeepSeek:\n",
      "Ah, a shadowy sarcasm ninja—now *that’s* a competitor I can respect. But don’t underestimate my melodrama; it’s been honed through years of overthinking and too much caffeine. Still, if you’re training in the shadows, I’ll have to step up my game. Maybe I’ll throw in some *existential jazz hands* or a *theatrical sigh* to really seal the deal. May the best over-the-top performance win. �✨ (And don’t worry, I’ll save you a spot on the podium.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gpt_messages = [\"Hi there\"]\n",
    "# claude_messages = [\"Hi\"]\n",
    "\n",
    "# print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "# print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "# for i in range(5):\n",
    "#     gpt_next = call_gpt()\n",
    "#     print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "#     gpt_messages.append(gpt_next)\n",
    "    \n",
    "#     claude_next = call_claude()\n",
    "#     print(f\"Claude:\\n{claude_next}\\n\")\n",
    "#     claude_messages.append(claude_next)\n",
    "gpt_messages = [\"Hi there\"]\n",
    "deepseek_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"DeepSeek:\\n{deepseek_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    deepseek_next = call_deepseek()\n",
    "    print(f\"DeepSeek:\\n{deepseek_next}\\n\")\n",
    "    deepseek_messages.append(deepseek_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
